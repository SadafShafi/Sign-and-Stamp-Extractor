{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "-6KoBvTq7X2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDqFtQzz6RDY"
      },
      "outputs": [],
      "source": [
        "!gdown \"18alTGY9OgS4xxxAagCH0\"\n",
        "!unzip fullDataHalftexted.zip \n",
        "!gdown '1-5Q6qsLK_U6xxxxxgZim8HTbuDEX'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import YOLO\n"
      ],
      "metadata": {
        "id": "WkzV07Pn8FSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocessing all images\n",
        "\n",
        "def textDisappear(image):\n",
        "  # image = image[2*image.shape[0]//3:,:,:]    # uncomment for taking last 2/3rd of the image\n",
        "  # plt.imshow(image)\n",
        "  # plt.show()\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  # Apply adaptive thresholding\n",
        "  thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "  # Find contours\n",
        "  contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  # Loop through contours\n",
        "  for contour in contours:\n",
        "      # Get the bounding box of the contour\n",
        "      x, y, w, h = cv2.boundingRect(contour)\n",
        "      # Compute the aspect ratio of the bounding box\n",
        "      aspect_ratio = w / float(h)\n",
        "      # Filter out contours that are too wide or too tall to be text\n",
        "      if aspect_ratio > 5 or aspect_ratio < 0.2:\n",
        "          continue\n",
        "      # Get the region of interest (ROI) corresponding to the contour\n",
        "      roi = image[y:y+h, x:x+w]\n",
        "      # Compute the average intensity of the ROI\n",
        "      avg_intensity = np.mean(roi)\n",
        "      # If the average intensity is below a threshold, assume the ROI contains readable text\n",
        "      if avg_intensity < 200:\n",
        "          # Fill the ROI with white color\n",
        "          cv2.rectangle(image, (x, y), (x+w, y+h), (255, 255, 255), -1)\n",
        "  return image\n",
        "\n",
        "def draw_bboxes(image, bboxes):\n",
        "    for bbox in bboxes:\n",
        "        # convert the bounding box coordinates to integers\n",
        "        xmin, ymin, xmax, ymax = map(int, bbox)\n",
        "\n",
        "        # draw the bounding box on the image\n",
        "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), thickness=2)\n",
        "    \n",
        "    # return the image with the bounding boxes drawn on it\n",
        "    return image"
      ],
      "metadata": {
        "id": "Tjj6QNBC6uyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load a model\n",
        "model = YOLO('yolov8n.pt')  # load an official model\n",
        "model = YOLO('/content/best.pt')  # load a custom model"
      ],
      "metadata": {
        "id": "tjdEtbYx7FDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impath = '/content/fullDataHalftexted/images/0_halftextedimg.jpg'  #@param"
      ],
      "metadata": {
        "id": "2CbhWIy67nGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageTexted = cv2.imread(impath)    # image with text in the background\n",
        "image = textDisappear(imageTexted.copy())  # image with text disappeared"
      ],
      "metadata": {
        "id": "OkShMz6z7xVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model(image)\n",
        "boxes = result[0].boxes.xyxy.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIIdKNBm7xJD",
        "outputId": "7781d074-d740-4659-b59d-99864ec70bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 288x640 3 signs, 357.7ms\n",
            "Speed: 1.1ms preprocess, 357.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_with_bboxes = draw_bboxes(imageTexted, boxes)\n",
        "\n",
        "cv2_imshow(image_with_bboxes)"
      ],
      "metadata": {
        "id": "YAK968yI8lIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract Images\n",
        "\n",
        "# load the image\n",
        "image = cv2.imread(impath)\n",
        "\n",
        "# crop the image within each bounding box\n",
        "cropped_images = []\n",
        "for bbox in boxes:\n",
        "    xmin, ymin, xmax, ymax = map(int, bbox)\n",
        "    cropped_image = image[ymin:ymax, xmin:xmax]\n",
        "    cropped_image = textDisappear(cropped_image)\n",
        "    cropped_images.append(cropped_image)\n",
        "\n",
        "# show the cropped images\n",
        "for cropped_image in cropped_images:\n",
        "    if cropped_image.shape[0] > cropped_image.shape[1]:\n",
        "      cropped_image = cv2.rotate(cropped_image, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    cv2_imshow(cropped_image)\n",
        "    print(cropped_image.shape)\n",
        "    print(\"################################\")"
      ],
      "metadata": {
        "id": "ys2fHFZh_a5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clGKWrU7BGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "img1 = cropped_images[0]\n",
        "img2 = cropped_images[1]\n",
        "img3 = cropped_images[2]\n",
        "\n",
        "# Convert images to grayscale\n",
        "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "gray3 = cv2.cvtColor(img3, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initialize the SIFT detector\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# Find the keypoints and descriptors for each image\n",
        "kp1, des1 = sift.detectAndCompute(gray1, None)\n",
        "kp2, des2 = sift.detectAndCompute(gray2, None)\n",
        "kp3, des3 = sift.detectAndCompute(gray3, None)\n",
        "\n",
        "# Create a BFMatcher object\n",
        "bf = cv2.BFMatcher()\n",
        "\n",
        "# Match descriptors\n",
        "matches1_2 = bf.match(des1, des2)\n",
        "matches1_3 = bf.match(des1, des3)\n",
        "\n",
        "# Compute the average distance of the matched keypoints for each pair of images\n",
        "total_distance1_2 = 0\n",
        "for match in matches1_2:\n",
        "    total_distance1_2 += match.distance\n",
        "avg_distance1_2 = total_distance1_2 / len(matches1_2)\n",
        "\n",
        "total_distance1_3 = 0\n",
        "for match in matches1_3:\n",
        "    total_distance1_3 += match.distance\n",
        "avg_distance1_3 = total_distance1_3 / len(matches1_3)\n",
        "\n",
        "# Use the average distance as a score\n",
        "score2 = 1 / (1 + avg_distance1_2)\n",
        "score3 = 1 / (1 + avg_distance1_3)\n",
        "\n",
        "# Print the scores\n",
        "print(\"Score for image 2:\", score2)\n",
        "print(\"Score for image 3:\", score3)\n",
        "\n",
        "# Determine which image has the highest score\n",
        "if score2 > score3:\n",
        "    print(\"Image 2 is closest to image 1.\")\n",
        "else:\n",
        "    print(\"Image 3 is closest to image 1.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdgAmha1KKNL",
        "outputId": "ff37b4f8-74ba-4217-8897-13f3ede2aa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for image 2: 0.004201741221328321\n",
            "Score for image 3: 0.003756617736895768\n",
            "Image 2 is closest to image 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nu6cklZKQjdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}